{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb291554",
   "metadata": {},
   "source": [
    "## üåÄ Diffusion DDPM with Contextual U-Net\n",
    "\n",
    " Welcome to this notebook on **Denoising Diffusion Probabilistic Models (DDPM)**. \n",
    "This guide will help you understand the structure and training of a diffusion model conditioned on context,\n",
    "applied here to a visual dataset like CLEVR.\n",
    "\n",
    "### üß† What you'll learn:\n",
    " - The principles behind DDPMs\n",
    " - How noise is added and removed from data over time\n",
    " - How a Contextual U-Net can condition image generation\n",
    " - The training pipeline and how to sample new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0d1f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "/home/lmb/dllab2025s/public/data/image_gen\n"
     ]
    }
   ],
   "source": [
    "# Imports and Device Setup\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import math \n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from argparse import Namespace\n",
    "from einops import rearrange, pack, unpack\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "from paths import CV_PATH_DDPM\n",
    "\n",
    "print(CV_PATH_DDPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eedd97f",
   "metadata": {},
   "source": [
    "# Parameters (Override as needed for interactive runs)\n",
    "These parameters define the behavior of training, model architecture, dataset, and sampling process.\n",
    "You can adjust them as needed to control the model‚Äôs performance.\n",
    "\n",
    "| Parameter | Description |\n",
    "|----------|-------------|\n",
    "| `lrate` | Learning rate |\n",
    "| `test_size` | Size of the object in the test dataset |\n",
    "| `num_samples` | Total number of samples |\n",
    "| `batch_size` | Number of images per training batch |\n",
    "| `n_T` | Number of diffusion steps |\n",
    "| `n_feat` | Feature size in U-Net |\n",
    "| `n_sample` | Number of samples during evaluation |\n",
    "| `n_epoch` | Number of training epochs |\n",
    "| `dataset` | Dataset name (e.g. CLEVR) |\n",
    "| `pixel_size` | Size of image (e.g. 28x28 pixels) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db37792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Namespace to emulate argparse arguments\n",
    "args = Namespace(\n",
    "    lrate=1e-4,\n",
    "    test_size=1.6,\n",
    "    num_samples=5000,\n",
    "    batch_size=64,\n",
    "    n_T=500,\n",
    "    n_feat=256,\n",
    "    n_sample=64,\n",
    "    n_epoch=100,\n",
    "    type_attention=\"\",\n",
    "    pixel_size=28,\n",
    "    dataset=\"clevr\",\n",
    "    seed=1,\n",
    "    save_model_path='comp_model.pth',\n",
    "    load_model_path=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a2443b",
   "metadata": {},
   "source": [
    "# Helper Functions and Schedules\n",
    "\n",
    "## üìê Diffusion Schedules\n",
    "\n",
    "The schedules define how noise is added to the data during the forward process.\n",
    "The reverse process attempts to recover clean data step-by-step.\n",
    "### üìä Key Variables\n",
    "- **`beta_t`**: Variance schedule controlling noise at each timestep\n",
    "- **`alpha_t`**: Data retention factor\n",
    "- **`alphabar_t`**: Cumulative product of `alpha_t`\n",
    "- **`sqrtab`, `sqrtmab`**: Precomputed factors used in forward/reverse diffusion\n",
    "\n",
    "This section also includes helper functions like:\n",
    "- `l2norm`: Normalize vectors using L2 norm\n",
    "- `exists`: Simple None check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce14bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2norm(t):\n",
    "    return F.normalize(t, dim=-1)\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# DDPM Schedules\n",
    "def ddpm_schedules(beta1, beta2, T):\n",
    "    assert beta1 < beta2 < 1.0, \"beta1 and beta2 must be in (0, 1)\"\n",
    "    beta_t = (beta2 - beta1) * torch.arange(0, T + 1, dtype=torch.float32) / T + beta1\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "    sqrtab = torch.sqrt(alphabar_t)\n",
    "    oneover_sqrta = 1 / torch.sqrt(alpha_t)\n",
    "    sqrtmab = torch.sqrt(1 - alphabar_t)\n",
    "    mab_over_sqrtmab = (1 - alpha_t) / sqrtmab\n",
    "    return {\n",
    "        'alpha_t': alpha_t,\n",
    "        'oneover_sqrta': oneover_sqrta,\n",
    "        'sqrt_beta_t': sqrt_beta_t,\n",
    "        'alphabar_t': alphabar_t,\n",
    "        'sqrtab': sqrtab,\n",
    "        'sqrtmab': sqrtmab,\n",
    "        'mab_over_sqrtmab': mab_over_sqrtmab,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e632a288",
   "metadata": {},
   "source": [
    "## üß± Model Components\n",
    "We now build the architecture of the DDPM using the following modules:\n",
    "### üîÅ Residual Blocks\n",
    "Help preserve information across layers and stabilize training.\n",
    "\n",
    "### üîç Attention Layers\n",
    "Add the ability to focus on spatial regions of interest. Optional, but helpful in U-Net-based models.\n",
    "\n",
    "### üß© U-Net Architecture\n",
    "A U-Net is used to predict the noise at each step of the reverse diffusion process.\n",
    "It consists of:\n",
    "- Downsampling path\n",
    "- Bottleneck\n",
    "- Upsampling path with skip connections\n",
    "- Embedding for conditioning on context/concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f349fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, is_res: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        '''\n",
    "        TODO:\n",
    "        Implement a standard ResNet-style convolutional block.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of channels in the input feature map.\n",
    "            out_channels (int): Number of channels produced by the block (also number of channels after 1st Conv2D layer).\n",
    "            is_res (bool): Whether to include a residual connection.\n",
    "\n",
    "        - Use two Conv2D layers with:\n",
    "            - kernel size = 3\n",
    "            - stride = 1\n",
    "            - padding = 1\n",
    "        - Each followed by BatchNorm and GELU activation.\n",
    "        - Track if in_channels == out_channels (used for skip connection logic).\n",
    "        '''\n",
    "\n",
    "        # Hint: you may want to store:\n",
    "        # self.same_channels\n",
    "        # self.is_res\n",
    "        # self.conv1 = nn.Sequential(...)\n",
    "        # self.conv2 = nn.Sequential(...)\n",
    "\n",
    "        self.same_channels = in_channels == out_channels\n",
    "        self.is_res = is_res\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(8, out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(8, out_channels),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.is_res:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            if self.same_channels:\n",
    "                out = x + x2\n",
    "            else:\n",
    "                out = x1 + x2 \n",
    "            return out\n",
    "        else:\n",
    "            x1 = self.conv1(x)\n",
    "            x2 = self.conv2(x1)\n",
    "            return x2\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class RearrangeToSequence(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b c ... -> b ... c')\n",
    "        x, ps = pack([x], 'b * c')\n",
    "\n",
    "        x = self.fn(x)\n",
    "\n",
    "        x, = unpack(x, ps, 'b * c')\n",
    "        x = rearrange(x, 'b ... c -> b c ...')\n",
    "        return x\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, dim, eps = 1e-5, fp16_eps = 1e-3, stable = False):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.fp16_eps = fp16_eps\n",
    "        self.stable = stable\n",
    "        self.g = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        eps = self.eps if x.dtype == torch.float32 else self.fp16_eps\n",
    "\n",
    "        if self.stable:\n",
    "            x = x / x.amax(dim = -1, keepdim = True).detach()\n",
    "\n",
    "        var = torch.var(x, dim = -1, unbiased = False, keepdim = True)\n",
    "        mean = torch.mean(x, dim = -1, keepdim = True)\n",
    "        return (x - mean) * (var + eps).rsqrt() * self.g\n",
    "\n",
    "        \n",
    "class Attention(nn.Module):\n",
    "    '''General implementation of Cross & Self Attention multi-head\n",
    "    '''\n",
    "    def __init__(self, embed_dim, hidden_dim, context_dim=None, num_heads=8, ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.context_dim = context_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.to_q = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "        if context_dim is None:\n",
    "            # Self Attention\n",
    "            self.to_k = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "            self.to_v = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "            self.self_attn = True\n",
    "        else:\n",
    "            # Cross Attention\n",
    "            self.to_k = nn.Linear(context_dim, embed_dim, bias=False)\n",
    "            self.to_v = nn.Linear(context_dim, embed_dim, bias=False)\n",
    "            self.self_attn = False\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim, bias=True)\n",
    "        )  # this could be omitted\n",
    "\n",
    "    def forward(self, tokens, context=None):\n",
    "        Q = self.to_q(tokens)\n",
    "        K = self.to_k(tokens) if self.self_attn else self.to_k(context)\n",
    "        V = self.to_v(tokens) if self.self_attn else self.to_v(context)\n",
    "        # print(Q.shape, K.shape, V.shape)\n",
    "        # transform heads onto batch dimension\n",
    "        Q = rearrange(Q, 'B T (H D) -> (B H) T D', H=self.num_heads, D=self.head_dim)\n",
    "        K = rearrange(K, 'B T (H D) -> (B H) T D', H=self.num_heads, D=self.head_dim)\n",
    "        V = rearrange(V, 'B T (H D) -> (B H) T D', H=self.num_heads, D=self.head_dim)\n",
    "        # print(Q.shape, K.shape, V.shape)\n",
    "        scoremats = torch.einsum(\"BTD,BSD->BTS\", Q, K)\n",
    "        attnmats = F.softmax(scoremats / math.sqrt(self.head_dim), dim=-1)\n",
    "        # print(scoremats.shape, attnmats.shape, )\n",
    "        ctx_vecs = torch.einsum(\"BTS,BSD->BTD\", attnmats, V)\n",
    "        # split the heads transform back to hidden.\n",
    "        ctx_vecs = rearrange(ctx_vecs, '(B H) T D -> B T (H D)', H=self.num_heads, D=self.head_dim)\n",
    "        # TODO: note this `to_out` is also a linear layer, could be in principle merged into the to_value layer.\n",
    "        return self.to_out(ctx_vecs)\n",
    "\n",
    "\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, type_attention):\n",
    "        super(UnetDown, self).__init__()\n",
    "        '''\n",
    "        process and downscale the image feature maps\n",
    "        '''\n",
    "        attention = nn.Identity()\n",
    "        if type_attention==\"self\":\n",
    "            create_self_attn = lambda dim: RearrangeToSequence(Residual(Attention(dim)))\n",
    "            attention = create_self_attn(out_channels)\n",
    "        self.model = nn.Sequential(*[ResidualConvBlock(in_channels, out_channels), attention, nn.MaxPool2d(2)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, type_attention):\n",
    "        super(UnetUp, self).__init__()\n",
    "        '''\n",
    "        process and upscale the image feature maps\n",
    "        '''\n",
    "        attention = nn.Identity()\n",
    "        if type_attention==\"self\": \n",
    "            create_self_attn = lambda dim: RearrangeToSequence(Residual(Attention(dim)))\n",
    "            attention = create_self_attn(out_channels)\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 2, 2),\n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "            attention, \n",
    "            ResidualConvBlock(out_channels, out_channels),\n",
    "        ]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class EmbedFC(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim):\n",
    "        super(EmbedFC, self).__init__()\n",
    "        '''\n",
    "        TODO:\n",
    "        Implement a simple feedforward embedding network.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimension of the input features.\n",
    "            emb_dim (int): Desired dimension of the embedding.\n",
    "\n",
    "        Design:\n",
    "        - First layer: Linear from input_dim ‚Üí emb_dim\n",
    "        - Activation: GELU\n",
    "        - Second layer: Linear from emb_dim ‚Üí emb_dim\n",
    "\n",
    "        Store the entire sequence in self.model using nn.Sequential.\n",
    "        '''\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(emb_dim, emb_dim),\n",
    "        )\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        TODO:\n",
    "        Flatten the input (if needed), and apply the embedding network.\n",
    "        - Pass it through self.model\n",
    "\n",
    "        Returns:\n",
    "            Embedded representation of shape [batch_size, emb_dim]\n",
    "        '''\n",
    "        x_in = x.view(-1, self.input_dim)\n",
    "        x_out = self.model(x_in)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "class ContextUnet(nn.Module):\n",
    "    def __init__(self, in_channels, n_feat = 256, n_classes=[2,3,1], type_attention=\"\"):\n",
    "        super(ContextUnet, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.n_contexts = len(n_classes)\n",
    "        self.n_feat = 2 * n_feat\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.init_conv = ResidualConvBlock(in_channels, n_feat, is_res=True)\n",
    "\n",
    "        self.down1 = UnetDown(n_feat, n_feat, type_attention)\n",
    "        self.down2 = UnetDown(n_feat, 2 * n_feat, type_attention)\n",
    "\n",
    "        self.to_vec = nn.Sequential(nn.AvgPool2d(7), nn.GELU())\n",
    "\n",
    "        self.timeembed1 = EmbedFC(1, 2*n_feat)\n",
    "        self.timeembed2 = EmbedFC(1, 1*n_feat)\n",
    "\n",
    "        ### embedding shape\n",
    "        self.n_out1 = 2*n_feat \n",
    "        self.n_out2 = n_feat\n",
    "\n",
    "        self.contextembed1 = nn.ModuleList([EmbedFC(self.n_classes[iclass], self.n_out1) for iclass in range(len(self.n_classes))])\n",
    "        self.contextembed2 = nn.ModuleList([EmbedFC(self.n_classes[iclass], self.n_out2) for iclass in range(len(self.n_classes))])\n",
    "\n",
    "        n_conv = 7\n",
    "        self.up0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * n_feat, 2 * n_feat, n_conv, n_conv), \n",
    "            nn.GroupNorm(8, 2 * n_feat),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.up1 = UnetUp(4 * n_feat, n_feat, type_attention)\n",
    "        self.up2 = UnetUp(2 * n_feat, n_feat, type_attention)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(2 * n_feat, n_feat, 3, 1, 1),\n",
    "            nn.GroupNorm(8, n_feat),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(n_feat, self.in_channels, 3, 1, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, c, t, context_mask=None):\n",
    "        '''\n",
    "        Forward pass of the ContextU-Net.\n",
    "    \n",
    "        Inputs:\n",
    "            x: Noisy input image (B x C x H x W)\n",
    "            c: List of context labels (e.g. color, shape, size)\n",
    "            t: Timestep tensor\n",
    "            context_mask: Optional mask to ignore parts of context\n",
    "    \n",
    "        Forward pass outline:\n",
    "        1. Pass x through init_conv, down1, down2\n",
    "        2. Compress features using to_vec\n",
    "        3. Embed timestep t using timeembed1 and timeembed2\n",
    "        4. Embed context c using contextembed1 and contextembed2 (see below)\n",
    "        5. Perform conditional upsampling (see below)\n",
    "        6. Output final image with skip connection from input x\n",
    "        '''\n",
    "        \n",
    "        # x is (noisy) image, c is context label, t is timestep, \n",
    "        x = self.init_conv(x)\n",
    "        down1 = self.down1(x)\n",
    "        down2 = self.down2(down1)\n",
    "        hiddenvec = self.to_vec(down2)\n",
    "    \n",
    "        temb1 = self.timeembed1(t).view(-1, int(self.n_feat), 1, 1)\n",
    "        temb2 = self.timeembed2(t).view(-1, int(self.n_feat/2), 1, 1)\n",
    "    \n",
    "        # embed context, time step\n",
    "        cemb1 = 0\n",
    "        cemb2 = 0\n",
    "        for ic in range(len(self.n_classes)):\n",
    "            tmpc = c[ic]\n",
    "            if tmpc.dtype==torch.int64: \n",
    "                tmpc = nn.functional.one_hot(tmpc, num_classes=self.n_classes[ic]).type(torch.float)\n",
    "            cemb1 += self.contextembed1[ic](tmpc).view(-1, int(self.n_out1/1.), 1, 1)\n",
    "            cemb2 += self.contextembed2[ic](tmpc).view(-1, int(self.n_out2/1.), 1, 1)\n",
    "    \n",
    "        up1 = self.up0(hiddenvec)\n",
    "        up2 = self.up1(cemb1*up1 + temb1, down2)\n",
    "        up3 = self.up2(cemb2*up2 + temb2, down1)\n",
    "        out = self.out(torch.cat((up3, x), 1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f8c7e",
   "metadata": {},
   "source": [
    "## DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aba5f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM(nn.Module):\n",
    "    def __init__(self, model, betas, n_T, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.n_T = n_T\n",
    "        for k,v in ddpm_schedules(betas[0], betas[1], n_T).items():\n",
    "            self.register_buffer(k, v)\n",
    "        self.mse = nn.MSELoss()\n",
    "    def forward(self, x, c_list):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        Implement the forward training pass of DDPM.\n",
    "\n",
    "        Objective:\n",
    "        Learn to predict the noise Œµ added to x_0 during the forward diffusion process.\n",
    "\n",
    "        Steps:\n",
    "        1. Sample a random timestep `t` ‚àà [1, T] for each sample in the batch use torch.randint.\n",
    "        2. Generate Gaussian noise Œµ ~ N(0, I).\n",
    "        3. Create noisy version of input image\n",
    "           x_t = sqrt(Œ±ÃÑ_t) * x + sqrt(1 - Œ±ÃÑ_t) * Œµ, hint: sqrt(Œ±ÃÑ_t), and sqrt(1 - Œ±ÃÑ_t) are parts of the precomputed tensors.\n",
    "        4. Predict Œµ using the model given (x_t, c_list, t), here t is the ratio between t and n_T.\n",
    "        5. Return MSE loss between predicted and true Œµ.\n",
    "\n",
    "        References:\n",
    "        üìö Lilian Weng's blog (forward process):\n",
    "        https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#forward-process\n",
    "        \"\"\"\n",
    "        t = torch.randint(1, self.n_T + 1, (x.shape[0],), device=self.device).long()\n",
    "\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "        sqrtab_t = self.sqrtab[t].view(-1, 1, 1, 1)\n",
    "        sqrtmab_t = self.sqrtmab[t].view(-1, 1, 1, 1)\n",
    "        x_t = sqrtab_t * x + sqrtmab_t * noise\n",
    "\n",
    "        t_ratio = t.float() / self.n_T\n",
    "\n",
    "        pred_noise = self.model(x_t, c_list, t_ratio)\n",
    "\n",
    "        loss = self.mse(pred_noise, noise)\n",
    "        return loss\n",
    "       \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n, c_list, size):\n",
    "        \"\"\"\n",
    "        TODO:\n",
    "        Implement the reverse sampling process of DDPM.\n",
    "\n",
    "        Objective:\n",
    "        Starting from pure noise x_T, iteratively denoise using the learned model.\n",
    "\n",
    "        Steps:\n",
    "        1. Sample x_T ~ N(0, I) according to n: number of samples and the size. \n",
    "        2. For each t in T to 1:\n",
    "           - create the t tensor, you can use torch.full\n",
    "           - Predict Œµ_t using the model given (x_t, c_list, t), t is the ratio between t and n_T.\n",
    "           - Compute x_{t-1} using the reverse formula:\n",
    "             x_{t-1} = 1/sqrt(Œ±_t) * (x_t - (1 - Œ±_t)/sqrt(1 - Œ±ÃÑ_t) * Œµ_t), hint:1/sqrt(Œ±_t), (1 - Œ±_t)/sqrt(1 - Œ±ÃÑ_t) are parts of the precomputed tensors.\n",
    "           - Unless t == 1: Add Gaussian noise g ~ N(0, 1) multiplied by sqrt(Œ≤_t), hint sqrt(Œ≤_t) is one of the precomputed tensors.\n",
    "             x_{t-1} += g * sqrt(Œ≤_t)\n",
    "        3. Return the final x.\n",
    "\n",
    "        References:\n",
    "        üìö Lilian Weng's blog (reverse process):\n",
    "        https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#reverse-process\n",
    "        \"\"\"\n",
    "        \n",
    "        C, H, W = size\n",
    "        x_t = torch.randn(n, C, H, W, device=self.device)\n",
    "        for i in tqdm(range(self.n_T, 0, -1)):\n",
    "            t = torch.full((n,), i, device=self.device).long()\n",
    "            t_ratio = t.float() / self.n_T\n",
    "\n",
    "            oneover_sqrta_t = self.oneover_sqrta[t].view(-1, 1, 1, 1)\n",
    "            mab_over_sqrtmab_t = self.mab_over_sqrtmab[t].view(-1, 1, 1, 1)\n",
    "\n",
    "            pred_noise = self.model(x_t, c_list, t_ratio)\n",
    "\n",
    "            x_t = oneover_sqrta_t * (x_t - mab_over_sqrtmab_t * pred_noise)\n",
    "\n",
    "            if i > 1:\n",
    "                noise = torch.randn_like(x_t)\n",
    "                sqrt_beta_t = self.sqrt_beta_t[t].view(-1, 1, 1, 1)\n",
    "                x_t += sqrt_beta_t * noise\n",
    "\n",
    "        return x_t\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dfaae9",
   "metadata": {},
   "source": [
    "## üß© CLEVR_dataset: Custom Dataset Loader for Conditional Diffusion\n",
    "\n",
    "This class loads and processes a subset of the [CLEVR dataset](https://cs.stanford.edu/people/jcjohns/clevr/) for training and evaluating conditional diffusion models.\n",
    "\n",
    "It supports:\n",
    " - üß™ Loading images and labels for **training** and **testing**\n",
    " - üîÅ Applying optional `transforms` (e.g. resizing, normalization)\n",
    " - üìé Parsing **contextual attributes** from accompanying `.json` files\n",
    "\n",
    "\n",
    "\n",
    " ### üîß Constructor: `__init__`\n",
    "\n",
    " **Parameters**:\n",
    " | Argument      | Description |\n",
    " |---------------|-------------|\n",
    " | `transform`   | Optional image transform (e.g. `transforms.Compose`) |\n",
    " | `num_samples` | Number of samples to return in total |\n",
    " | `dataset`     | Dataset name (used in file path) |\n",
    " | `configs`     | List of shape-color-size combinations to include |\n",
    " | `training`    | Whether to load training or test split |\n",
    " | `test_size`   | Optional override for object size during test sampling |\n",
    "\n",
    " Based on `training`, it collects image paths from:\n",
    " - `{CV_PATH_DDPM}/{dataset}/train/...` for training\n",
    " - `{CV_PATH_DDPM}/{dataset}/test/...` for testing\n",
    "\n",
    "\n",
    " ### üñºÔ∏è `__getitem__`: Loading and Labeling an Image\n",
    "\n",
    " 1. **Randomly selects** an image path from the collected list\n",
    " 2. **Loads the RGB image** using `PIL`\n",
    " 3. **Applies transform** if provided\n",
    " 4. **Parses metadata** from the filename and corresponding `.json` file\n",
    "\n",
    " - JSON file includes:\n",
    "   - `size`: scalar (float)\n",
    "   - `color`: RGB array\n",
    "\n",
    " - The filename includes a label string formatted like `shape_color_size`\n",
    "\n",
    " **Returns:**\n",
    " - `img`: preprocessed image tensor\n",
    " - `label`: dictionary of conditioning variables, where:\n",
    "   ```python\n",
    "   label = {\n",
    "       0: shape (int),\n",
    "       1: color (np.array of shape [3]),\n",
    "       2: size (float as np.array)\n",
    "   }\n",
    "   ```\n",
    "\n",
    "\n",
    " ### üìè `__len__`: Number of Samples\n",
    "\n",
    " Returns the predefined `num_samples`, which can be different from the actual dataset size (used for random sampling).\n",
    "#\n",
    "\n",
    "#\n",
    " ‚úÖ **Why this design?**\n",
    " - Enables **context-conditioned generation** (shape, color, size)\n",
    " - General enough to extend to other CLEVR-like datasets\n",
    " - Flexible random access & transformable\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "190ec440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLEVR_dataset(Dataset):\n",
    "    def __init__(self, transform=None, num_samples=5000, dataset=\"\", configs=\"\", training=True, test_size=None):\n",
    "        self.training = training\n",
    "        self.test_size = test_size\n",
    "        self.dataset = dataset\n",
    "\n",
    "        prefix =\"CLEVR\"\n",
    "        ext = \".png\"\n",
    "        \n",
    "        if training:\n",
    "            self.train_image_paths = []\n",
    "            for config in configs:\n",
    "                new_paths = glob.glob(f\"{CV_PATH_DDPM}/{dataset}/train/{prefix}_{config}_*{ext}\")\n",
    "                self.train_image_paths.extend(new_paths)\n",
    "            self.len_data = len(self.train_image_paths)\n",
    "        else:\n",
    "            self.test_image_paths = glob.glob(f\"{CV_PATH_DDPM}/{dataset}/test/{prefix}_{configs}_*{ext}\")\n",
    "            self.len_data = len(self.test_image_paths)\n",
    "\n",
    "        self.num_samples = num_samples\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.training:\n",
    "           ipath = random.randint(0, len(self.train_image_paths)-1)\n",
    "           img_path = self.train_image_paths[ipath]\n",
    "        else:\n",
    "           ipath = random.randint(0, len(self.test_image_paths)-1)\n",
    "           img_path = self.test_image_paths[ipath]\n",
    "            \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "           img = self.transform(img)\n",
    "   \n",
    "        name_labels = img_path.split(\"_\")[-2]\n",
    "\n",
    "        with open(img_path.replace(\".png\", \".json\"), 'r') as f:\n",
    "            my_dict = json.loads(f.read())\n",
    "            _size = my_dict[0]\n",
    "            _color = my_dict[1][:3]\n",
    "    \n",
    "        if self.training:\n",
    "            size, color = _size, _color\n",
    "        else:\n",
    "            # Define colors mapping\n",
    "            colors_map = {\n",
    "                '0': [0.9, 0.1, 0.1],\n",
    "                '1': [0.1, 0.1, 0.9],\n",
    "                '2': [0.1, 0.9, 0.1]\n",
    "            }\n",
    "            # Assign size and color based on label values\n",
    "            size = 2.6 if int(name_labels[2]) == 0 else self.test_size\n",
    "            color = colors_map[name_labels[1]]\n",
    "    \n",
    "        # Convert size and color to numpy arrays\n",
    "        size = np.array(size, dtype=np.float32)\n",
    "        color = np.array(color, dtype=np.float32)\n",
    "    \n",
    "        # Create the label dictionary\n",
    "        label = {0: int(name_labels[0]), 1: color, 2: size}\n",
    "       \n",
    "\n",
    "        return img, label \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d2da338-0159-4093-8abc-45123ef31fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: array([0.8715985 , 0.1768101 , 0.18810451], dtype=float32),\n",
       " 2: array(2.2930193, dtype=float32)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# VISUALIZE DATASET\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "configs = [\"000\",\"001\",\"100\",\"010\"]\n",
    "tf = transforms.Compose([transforms.Resize((args.pixel_size,args.pixel_size)), transforms.ToTensor()])\n",
    "train_dataset = CLEVR_dataset(tf, args.num_samples, args.dataset, configs=[\"000\",\"001\",\"100\",\"010\"], training=True) \n",
    "img, label = train_dataset[0]\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2728b13",
   "metadata": {},
   "source": [
    "# Training and Sampling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b598588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(args):\n",
    "    torch.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    configs = [\"000\",\"001\",\"100\",\"010\"]\n",
    "    tf = transforms.Compose([transforms.Resize((args.pixel_size,args.pixel_size)), transforms.ToTensor()])\n",
    "    model = DDPM(ContextUnet(3,args.n_feat,[2,3,1],args.type_attention), (args.lrate,0.02), args.n_T, device)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=args.lrate)\n",
    "    train_dataset = CLEVR_dataset(tf, args.num_samples, args.dataset, configs=[\"000\",\"001\",\"100\",\"010\"], training=True) \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    epoch_losses = []\n",
    "    epoch = []\n",
    "    for ep in tqdm(range(args.n_epoch), desc=\"Training Epochs\"):\n",
    "        model.train()\n",
    "        losses = []\n",
    "\n",
    "        # Wrapping loader with tqdm to track batch progress\n",
    "        for x, c in tqdm(train_dataloader, desc=f\"Epoch {ep+1}\", leave=False):\n",
    "            x = x.to(device)\n",
    "            cl = [c[i].to(device) for i in c]\n",
    "            # save first 10 images in x to debug using grid vu grid\n",
    "            if ep==0 and len(losses)==0:\n",
    "                grid = vutils.make_grid(x[:10], nrow=5, normalize=True)\n",
    "                plt.imshow(grid.permute(1, 2, 0).cpu())\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "            \n",
    "            # TODO\n",
    "            # 1. zero gradients\n",
    "            # 2. calc loss with model(...)\n",
    "            # 3. backprogate loss \n",
    "            # 4. optimize parameters for single step\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss = model(x, cl)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        tqdm.write(f\"Epoch {ep+1}/{args.n_epoch}, Loss: {np.mean(losses):.4f}\")\n",
    "        epoch_losses.append(np.mean(losses))\n",
    "        epoch.append(ep+1)\n",
    "        \n",
    "    return model, epoch_losses, epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5790f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(model, args, config=\"000\"):\n",
    "    model.eval()\n",
    "    tf = transforms.Compose([transforms.Resize((args.pixel_size,args.pixel_size)), transforms.ToTensor()])\n",
    "    test_dataset = CLEVR_dataset(\n",
    "        tf, args.n_sample,\n",
    "        args.dataset,\n",
    "        configs=config,\n",
    "        training=False,\n",
    "        test_size=args.test_size,\n",
    "        )\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                            batch_size=args.batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=1)\n",
    "    x_real, c_gen = next(iter(test_loader))\n",
    "    cl = {k: v[:args.n_sample].to(device) for k, v in c_gen.items()}\n",
    "    gen = model.sample(args.n_sample, cl, (3,args.pixel_size,args.pixel_size))\n",
    "    return gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4f5c7",
   "metadata": {},
   "source": [
    "# Quantitative measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddfa1da",
   "metadata": {},
   "source": [
    " ## üß† MLP Probe for Context Prediction\n",
    "\n",
    " This section defines a simple **multi-head MLP classifier** used to evaluate whether generated images retain the correct **contextual information** (shape, color, size).\n",
    "\n",
    " ### üß© MLP Architecture\n",
    " - Takes a flattened image as input.\n",
    " - Outputs three predictions via separate linear heads:\n",
    "   - `0`: shape (3 classes)\n",
    "   - `1`: color (3 classes)\n",
    "   - `2`: size (2 classes)\n",
    "\n",
    "# ### üìä Accuracy Evaluation\n",
    " The `calc_acc` function:\n",
    " - Passes generated images through the MLP probe.\n",
    " - Computes prediction accuracy for each context attribute.\n",
    " - Used to **quantitatively assess how well the model preserves conditioning** during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a3ecc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_fc0 = nn.Linear(input_dim, output_dims[0])\n",
    "        self.output_fc1 = nn.Linear(input_dim, output_dims[1])\n",
    "        self.output_fc2 = nn.Linear(input_dim, output_dims[2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x[:,:3,:,:].reshape(batch_size, -1)\n",
    "\n",
    "        y_pred = {}\n",
    "        y_pred[0] = self.output_fc0(x)\n",
    "        y_pred[1] = self.output_fc1(x)\n",
    "        y_pred[2] = self.output_fc2(x)\n",
    "\n",
    "        return y_pred \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6e0c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_input_dim = args.pixel_size * args.pixel_size * 3\n",
    "classifier_linear = MLP(probe_input_dim, [3,3,2,2])\n",
    "classifier_linear.load_state_dict(torch.load(f\"{CV_PATH_DDPM}/probe.pt\", map_location=torch.device(device)))\n",
    "classifier_linear = classifier_linear.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c27a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(preds, obs, classifier, nclasses=3):\n",
    "    y_pred = classifier(preds) #(torch.from_numpy(preds).to(device))\n",
    "    accs = []\n",
    "    preds = []\n",
    "    for ii in range(nclasses): \n",
    "        top_pred = y_pred[ii].argmax(1, keepdim=True).detach().cpu().numpy()\n",
    "        acc = np.array(top_pred[:,0]==int(obs[ii]), dtype=np.int64) \n",
    "        accs.append( acc ) \n",
    "        preds.append( top_pred[:,0] ) \n",
    "    return np.array(preds), np.array(accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e4f1b",
   "metadata": {},
   "source": [
    "# Run Training or Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa42b99",
   "metadata": {},
   "source": [
    "losses should be decreasing and going below 0.2 after 1 epoch and below 0.005 after 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbd9b577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   1%|          | 1/100 [00:53<1:28:54, 53.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.2153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   2%|‚ñè         | 2/100 [01:30<1:11:48, 43.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Loss: 0.0577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   3%|‚ñé         | 3/100 [02:07<1:05:45, 40.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Loss: 0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   4%|‚ñç         | 4/100 [02:44<1:02:35, 39.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Loss: 0.0283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   5%|‚ñå         | 5/100 [03:21<1:00:35, 38.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 0.0211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   6%|‚ñå         | 6/100 [03:58<59:15, 37.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Loss: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   7%|‚ñã         | 7/100 [04:34<58:08, 37.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Loss: 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   8%|‚ñä         | 8/100 [05:11<57:10, 37.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Loss: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:   9%|‚ñâ         | 9/100 [05:48<56:20, 37.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  10%|‚ñà         | 10/100 [06:25<55:35, 37.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  11%|‚ñà         | 11/100 [07:02<54:53, 37.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  12%|‚ñà‚ñè        | 12/100 [07:39<54:13, 36.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Loss: 0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  13%|‚ñà‚ñé        | 13/100 [08:16<53:34, 36.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Loss: 0.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  14%|‚ñà‚ñç        | 14/100 [08:53<52:56, 36.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Loss: 0.0102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  15%|‚ñà‚ñå        | 15/100 [09:29<52:17, 36.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Loss: 0.0097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  16%|‚ñà‚ñå        | 16/100 [10:06<51:39, 36.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Loss: 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  17%|‚ñà‚ñã        | 17/100 [10:43<51:01, 36.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Loss: 0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  18%|‚ñà‚ñä        | 18/100 [11:20<50:24, 36.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Loss: 0.0082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  19%|‚ñà‚ñâ        | 19/100 [11:57<49:45, 36.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Loss: 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  20%|‚ñà‚ñà        | 20/100 [12:34<49:08, 36.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  21%|‚ñà‚ñà        | 21/100 [13:10<48:30, 36.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Loss: 0.0074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  22%|‚ñà‚ñà‚ñè       | 22/100 [13:47<47:53, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Loss: 0.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  23%|‚ñà‚ñà‚ñé       | 23/100 [14:24<47:16, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, Loss: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  24%|‚ñà‚ñà‚ñç       | 24/100 [15:01<46:39, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Loss: 0.0065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  25%|‚ñà‚ñà‚ñå       | 25/100 [15:38<46:02, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Loss: 0.0071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  26%|‚ñà‚ñà‚ñå       | 26/100 [16:15<45:25, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, Loss: 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  27%|‚ñà‚ñà‚ñã       | 27/100 [16:51<44:48, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100, Loss: 0.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  28%|‚ñà‚ñà‚ñä       | 28/100 [17:28<44:12, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Loss: 0.0061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  29%|‚ñà‚ñà‚ñâ       | 29/100 [18:05<43:37, 36.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, Loss: 0.0076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  30%|‚ñà‚ñà‚ñà       | 30/100 [18:42<42:59, 36.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Loss: 0.0059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  31%|‚ñà‚ñà‚ñà       | 31/100 [19:19<42:22, 36.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100, Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [19:56<41:47, 36.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100, Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  33%|‚ñà‚ñà‚ñà‚ñé      | 33/100 [20:33<41:10, 36.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  34%|‚ñà‚ñà‚ñà‚ñç      | 34/100 [21:10<40:33, 36.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100, Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  35%|‚ñà‚ñà‚ñà‚ñå      | 35/100 [21:46<39:56, 36.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100, Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [22:23<39:19, 36.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100, Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/100 [23:00<38:41, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100, Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  38%|‚ñà‚ñà‚ñà‚ñä      | 38/100 [23:37<38:04, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100, Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  39%|‚ñà‚ñà‚ñà‚ñâ      | 39/100 [24:14<37:25, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100, Loss: 0.0049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [24:51<36:49, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [25:27<36:13, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100, Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 42/100 [26:04<35:36, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100, Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 43/100 [26:41<34:59, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100, Loss: 0.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [27:18<34:24, 36.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100, Loss: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 45/100 [27:55<33:46, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100, Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 46/100 [28:32<33:09, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100, Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [29:08<32:32, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100, Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [29:45<31:56, 36.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100, Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 49/100 [30:22<31:18, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100, Loss: 0.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [30:59<30:41, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 51/100 [31:36<30:04, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100, Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/100 [32:13<29:27, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100, Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [32:49<28:50, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100, Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 54/100 [33:26<28:13, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100, Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 55/100 [34:03<27:36, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100, Loss: 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [34:40<26:59, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100, Loss: 0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [35:17<26:23, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100, Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 58/100 [35:54<25:47, 36.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100, Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 59/100 [36:30<25:10, 36.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100, Loss: 0.0037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [37:07<24:34, 36.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Loss: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 61/100 [37:44<23:56, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100, Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 62/100 [38:21<23:19, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100, Loss: 0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 63/100 [38:58<22:42, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100, Loss: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 64/100 [39:35<22:05, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100, Loss: 0.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [40:11<21:29, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100, Loss: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 66/100 [40:48<20:52, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100, Loss: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 67/100 [41:25<20:15, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100, Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [42:02<19:39, 36.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100, Loss: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 69/100 [42:39<19:01, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100, Loss: 0.0039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/100 [43:16<18:24, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100, Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 71/100 [43:52<17:48, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100, Loss: 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [44:29<17:11, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100, Loss: 0.0040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 73/100 [45:06<16:34, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100, Loss: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 74/100 [45:43<15:57, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100, Loss: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [46:20<15:20, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100, Loss: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 76/100 [46:56<14:43, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100, Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [47:33<14:06, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100, Loss: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 78/100 [48:10<13:29, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100, Loss: 0.0043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 79/100 [48:47<12:52, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100, Loss: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [49:24<12:16, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Loss: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 81/100 [50:00<11:39, 36.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100, Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 82/100 [50:37<11:02, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Loss: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [51:14<10:25, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100, Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 84/100 [51:51<09:49, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100, Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 85/100 [52:28<09:12, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100, Loss: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 86/100 [53:05<08:35, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100, Loss: 0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [53:41<07:58, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100, Loss: 0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 88/100 [54:18<07:22, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100, Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [54:55<06:45, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100, Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [55:32<06:08, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100, Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/100 [56:09<05:31, 36.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100, Loss: 0.0036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 92/100 [56:46<04:54, 36.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100, Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [57:22<04:17, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100, Loss: 0.0035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 94/100 [57:59<03:40, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100, Loss: 0.0031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 95/100 [58:36<03:04, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100, Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [59:13<02:27, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100, Loss: 0.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 97/100 [59:50<01:50, 36.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100, Loss: 0.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 98/100 [1:00:26<01:13, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100, Loss: 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 99/100 [1:01:03<00:36, 36.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100, Loss: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [1:01:40<00:00, 37.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Loss: 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary()\n",
    "\n",
    "model, epoch_losses, epochs = train_model(args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01616d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, epoch_losses, label='Average Epoch Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdd9d090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to comp_model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), args.save_model_path)\n",
    "print(f\"Model saved to {args.save_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da627262",
   "metadata": {},
   "source": [
    "# Sampling, Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf5995",
   "metadata": {},
   "source": [
    "loading the model that was trained before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06a2aee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from comp_model.pth\n"
     ]
    }
   ],
   "source": [
    "n_classes = [2, 3, 1]\n",
    "\n",
    "# Build model\n",
    "ddpm = DDPM(\n",
    "    ContextUnet(\n",
    "        in_channels=3,\n",
    "        n_feat=args.n_feat,\n",
    "        n_classes=n_classes,\n",
    "        type_attention=args.type_attention\n",
    "    ),\n",
    "    betas=(args.lrate, 0.02),\n",
    "    n_T=args.n_T,\n",
    "    device=device,\n",
    ").to(device)\n",
    "ddpm.eval()\n",
    "# Load weights\n",
    "\n",
    "state = torch.load(args.save_model_path, map_location=device)\n",
    "ddpm.load_state_dict(state)\n",
    "ddpm = ddpm.to(device)\n",
    "print(f\"Loaded model from {args.save_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5211cbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample images from all configs\n",
    "#create subplots for each config\n",
    "import torchvision.utils as vutils\n",
    "nrows = 4\n",
    "ncols = 1\n",
    "\n",
    "acc = {}\n",
    "sample_train_sources = [\"000\",\"001\",\"100\",\"010\"] # configurations to sample from that was seen during the training\n",
    "for config in sample_train_sources:\n",
    "       # Plot the generated images\n",
    "\n",
    "   gen = sample_images(ddpm, args, config)\n",
    "   #clip gen to 0-1\n",
    "   gen = torch.clamp(gen, 0, 1)\n",
    "\n",
    "   # calc acc\n",
    "   preds, accs = calc_acc(gen, config, classifier_linear, nclasses=3)\n",
    "   acc[config] = accs.mean()\n",
    "   print(f\"Accuracy for config {config}: {acc[config]}\")\n",
    "\n",
    "   grid = vutils.make_grid(gen.cpu(), nrow=nrows, normalize=False)\n",
    "\n",
    "   plt.figure(figsize=(nrows, ncols))\n",
    "   plt.axis(\"off\")\n",
    "   plt.title(f\"Generated Images: {config}\")\n",
    "   plt.imshow(grid.permute(1, 2, 0))  # CxHxW -> HxWxC\n",
    "   plt.show()\n",
    "   vutils.save_image(gen.cpu(), f\"generated_images_{config}.png\", nrow=nrows, normalize=False)\n",
    "   print(f\"Generated images for config {config} saved as generated_images_{config}.png\")\n",
    "\n",
    "   #now save mean image\n",
    "   mean_image = gen.mean(dim=0)        \n",
    "   mean_image = torch.clamp(mean_image, 0, 1)\n",
    "   grid = vutils.make_grid(mean_image.cpu(), nrow=nrows, normalize=False)\n",
    "   plt.figure(figsize=(nrows, ncols))\n",
    "   plt.axis(\"off\")\n",
    "   plt.title(f\"Mean Image: {config}\")\n",
    "   plt.imshow(grid.permute(1, 2, 0))  # CxHxW -> HxWxC\n",
    "   plt.show()\n",
    "   vutils.save_image(mean_image.cpu(), f\"mean_image_{config}.png\", nrow=nrows, normalize=False)\n",
    "   print(f\"Mean image for config {config} saved as mean_image_{config}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e281481",
   "metadata": {},
   "source": [
    " ## üîç Generalization: Sampling from Unseen Contexts\n",
    "\n",
    " So far, we‚Äôve only sampled from **training configurations** like `000`, `001`, etc.\n",
    " But what happens when we ask the model to generate from **unseen combinations** of shape, color, and size?\n",
    "\n",
    " > ü§î **Big Question:**  \n",
    " > Can a diffusion model generalize to novel combinations of known attributes it has never seen together during training?\n",
    "\n",
    "# In this next cell, you will:\n",
    " - Define a set of **unseen context configurations**\n",
    " - Generate images from the model using those contexts\n",
    " - Evaluate and **visually inspect** if the model understands the disentangled structure of the conditioning\n",
    " - Check the **accuracy** of the generated images using the MLP probe\n",
    " ‚úçÔ∏è Try it yourself below!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code here\n",
    "\n",
    "unseen_configs = [\"011\",\"110\",\"101\",\"111\"]\n",
    "\n",
    "acc_unseen = {}\n",
    "\n",
    "for config in unseen_configs:\n",
    "    # Plot the generated images\n",
    "    gen = sample_images(ddpm, args, config)\n",
    "    #clip gen to 0-1\n",
    "    gen = torch.clamp(gen, 0, 1)\n",
    "\n",
    "    # calc acc\n",
    "    preds, accs = calc_acc(gen, config, classifier_linear, nclasses=3)\n",
    "    acc_unseen[config] = accs.mean()\n",
    "    print(f\"Accuracy for unseen config {config}: {acc_unseen[config]}\")\n",
    "\n",
    "    grid = vutils.make_grid(gen.cpu(), nrow=nrows, normalize=False)\n",
    "\n",
    "    plt.figure(figsize=(nrows, ncols))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Generated Images: {config}\")\n",
    "    plt.imshow(grid.permute(1, 2, 0))  # CxHxW -> HxWxC\n",
    "    plt.show()\n",
    "    vutils.save_image(gen.cpu(), f\"generated_images_{config}.png\", nrow=nrows, normalize=False)\n",
    "    print(f\"Generated images for unseen config {config} saved as generated_images_{config}.png\")\n",
    "\n",
    "    #now save mean image\n",
    "    mean_image = gen.mean(dim=0)\n",
    "    mean_image = torch.clamp(mean_image, 0, 1)\n",
    "    grid = vutils.make_grid(mean_image.cpu(), nrow=nrows, normalize=False)\n",
    "    plt.figure(figsize=(nrows, ncols))\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Mean Image: {config}\")\n",
    "    plt.imshow(grid.permute(1, 2, 0))  # CxHxW -> HxWxC\n",
    "    plt.show()\n",
    "    vutils.save_image(mean_image.cpu(), f\"mean_image_{config}.png\", nrow=nrows, normalize=False)\n",
    "    print(f\"Mean image for unseen config {config} saved as mean_image_{config}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2964e",
   "metadata": {},
   "source": [
    "# For more fun experiments try different configurations like adding Attention layers, changing the number of diffusion steps, or using different datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
